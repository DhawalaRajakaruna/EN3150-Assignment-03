{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3703bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e3c1c",
   "metadata": {},
   "source": [
    "## **Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8970f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 3 layer covolution model \n",
    "class CNN_model(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolution blocks\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 16, kernel_size=5, stride=1,padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # NN block\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(32*32*64, 1024),     # 65,536 -> 1024\n",
    "            nn.BatchNorm1d(1024),          # Add BatchNorm for regularization\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),               # Add dropout to prevent overfitting\n",
    "            nn.Linear(1024, 512),          # Add an additional layer for deeper representation\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),           # Another layer for further abstraction\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 9)              # Output layer (9 waste categories)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        X = self.feature_extractor(x)\n",
    "        X = torch.flatten(X,1)\n",
    "        X = self.fully_connected(X)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dad593",
   "metadata": {},
   "source": [
    "## **Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74647608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 4752\n",
      "Class names: ['Cardboard', 'Food Organics', 'Glass', 'Metal', 'Miscellaneous Trash', 'Paper', 'Plastic', 'Textile Trash', 'Vegetation']\n",
      "Original training samples: 3326\n",
      "Augmented training samples: 13304 (Ã—4.0)\n",
      "Validation samples: 712\n",
      "Test samples: 714\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import random\n",
    "\n",
    "class AugmentedDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper that applies multiple augmentations to create additional samples\"\"\"\n",
    "    def __init__(self, dataset, num_augmentations=3, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.num_augmentations = num_augmentations\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Basic transform for converting to tensor if no transform provided\n",
    "        self.basic_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Define a set of stronger augmentation transforms\n",
    "        self.augmentations = [\n",
    "            # Transform 1: Horizontal flip + slight rotation\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            # Transform 2: Vertical flip + color jitter\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomVerticalFlip(p=1.0),\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            # Transform 3: Random affine + color jitter\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            # Transform 4: Center crop + rotation\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((300, 300)),  # Larger size for crop\n",
    "                transforms.CenterCrop(256),\n",
    "                transforms.RandomRotation(30),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            # Transform 5: Gaussian blur + contrast\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "                transforms.ColorJitter(contrast=0.4),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * (self.num_augmentations + 1)  # Original + augmentations\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which original image to use\n",
    "        original_idx = idx // (self.num_augmentations + 1)\n",
    "        aug_idx = idx % (self.num_augmentations + 1)\n",
    "        \n",
    "        # Get original image\n",
    "        img, label = self.dataset[original_idx]\n",
    "        \n",
    "        # If aug_idx is 0, return original with basic transform\n",
    "        if aug_idx == 0:\n",
    "            if self.transform:\n",
    "                return self.transform(img), label\n",
    "            else:\n",
    "                return self.basic_transform(img), label\n",
    "        \n",
    "        # Otherwise, apply one of the augmentations\n",
    "        # Use modulo to ensure we stay within the range of available augmentations\n",
    "        transform_idx = (aug_idx - 1) % len(self.augmentations)\n",
    "        transform = self.augmentations[transform_idx]\n",
    "        return transform(img), label\n",
    "\n",
    "# Define standard transform for basic transformations\n",
    "transform_standard = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define transform for validation/test (no augmentation)\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Path to the RealWaste dataset\n",
    "data_dir = \"realwaste-main/RealWaste\"  # Update if needed\n",
    "\n",
    "# Load the original dataset without transforms (we'll apply them in the wrapper)\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=None)\n",
    "\n",
    "# Get dataset size\n",
    "dataset_size = len(dataset)\n",
    "print(f\"Original dataset size: {dataset_size}\")\n",
    "print(f\"Class names: {dataset.classes}\")\n",
    "\n",
    "# Calculate split sizes (70% train, 15% validation, 15% test)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.15 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# Create indices for the splits\n",
    "indices = list(range(dataset_size))\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "# Create custom datasets with appropriate transforms and indices\n",
    "class CustomSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[self.indices[idx]]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "# Create base dataset splits\n",
    "train_subset = CustomSubset(dataset, train_indices, transform=None)  # Apply transformation later\n",
    "val_dataset = CustomSubset(dataset, val_indices, transform=transform_val)\n",
    "test_dataset = CustomSubset(dataset, test_indices, transform=transform_val)\n",
    "\n",
    "# Augment only the training set (4 augmentations per image + original)\n",
    "train_dataset = AugmentedDataset(train_subset, num_augmentations=3, transform=transform_standard)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 256  # Smaller batch size for larger images\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Print split information\n",
    "original_train_size = len(train_subset)\n",
    "augmented_train_size = len(train_dataset)\n",
    "print(f\"Original training samples: {original_train_size}\")\n",
    "print(f\"Augmented training samples: {augmented_train_size} (Ã—{augmented_train_size / original_train_size:.1f})\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ee125",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa769b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def Train(device, model, epochs, save_dir=\"models\", batch_size=None):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    # Optional: Add learning rate scheduler for better convergence\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    EPOCHS = epochs\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "    last_model_path = os.path.join(save_dir, \"last_model.pth\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "        for images, labels in train_loader:\n",
    "            # Move data to device\n",
    "            print(\"in the epoch\")\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            print(\"grad zero\")\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            print(\"sent to model\")\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            print(\"Loss backward\")\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics (using your approach)\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_loss /= len(train_dataset)\n",
    "        train_acc = train_correct / len(train_dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                # Move data to device\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                \n",
    "                # Track metrics\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                predicted = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_loss /= len(val_dataset)\n",
    "        val_acc = val_correct / len(val_dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save the latest model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_losses,\n",
    "            'train_acc': train_accs,\n",
    "            'val_loss': val_losses,\n",
    "            'val_acc': val_accs,\n",
    "            'current_lr': optimizer.param_groups[0]['lr']\n",
    "        }, last_model_path)\n",
    "        \n",
    "        # Save the best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_losses,\n",
    "                'train_acc': train_accs,\n",
    "                'val_loss': val_losses,\n",
    "                'val_acc': val_accs,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'current_lr': optimizer.param_groups[0]['lr']\n",
    "            }, best_model_path)\n",
    "            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        # Calculate time taken for epoch\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # Print epoch summary (using your format)\n",
    "        print(f'Epoch: {epoch+1}/{EPOCHS}, '\n",
    "              f'Time: {epoch_time:.2f}s, '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.4f}, '\n",
    "              f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    print(f\"Training completed. Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best model saved to {best_model_path}\")\n",
    "    print(f\"Last model saved to {last_model_path}\")\n",
    "    \n",
    "    # Return training history\n",
    "    history = {\n",
    "        'train_loss': train_losses,\n",
    "        'train_acc': train_accs,\n",
    "        'val_loss': val_losses,\n",
    "        'val_acc': val_accs\n",
    "    }\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37877ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "987b4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_model(in_channel=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea888d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set scheduler\n",
      "Starting training...\n",
      "Epoch 1/30 [Train]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ML\\envs\\torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "history, trained_model = Train(device, model, epochs=30, save_dir=\"models\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
